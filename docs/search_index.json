[["index.html", "Ethical Data Handling About this course 0.1 Available course formats", " Ethical Data Handling 2025-02-21 About this course This course is part of a series of courses created for the Informatics Technology for Cancer Research (ITCR) Training Network (ITN). The ITN is a collaborative effort of researchers around the United States that supports cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at itcrtraining.org for more information. Except where otherwise indicated, the contents of this course are available for use under the Creative Commons Attribution 4.0 license. You are free to adapt and share the work, but you must give appropriate credit, provide a link to the license, and indicate if changes were made. Sample attribution: Ethical Data Handling for Cancer Research by Fred Hutchinson Data Science Lab (CC-BY 4.0). You can download the illustrations by clicking here. 0.1 Available course formats This course is available in multiple formats which allows you to take it in the way that best suites your needs. You can take it for certificate which can be for free or fee. The material for this course can be viewed without login requirement on this Bookdown website. This format might be most appropriate for you if you rely on screen-reader technology. This course can be taken for free certification through Leanpub. This course can be taken on Coursera for certification here (but it is not available for free on Coursera). Our courses are open source, you can find the source material for this course on GitHub. "],["introduction.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Target Audience 1.3 Topics Covered 1.4 Curriculum", " Chapter 1 Introduction 1.1 Motivation One of the key cancer informatics challenges is dealing with and managing the explosion of data from multiple sources. This course is designed to help researchers and investigators to understand the key ethical principles of data management from a privacy, security, usability and discoverability perspective. 1.2 Target Audience This course is intended for researchers (including postdocs and students) with limited to intermediate experience with informatics research. The conceptual material will also be useful for those in management roles who are collecting data and using informatics pipelines. 1.3 Topics Covered 1.4 Curriculum The course will cover key underlying principles and concepts in ethical data handling. It will also cover how best practices for data security, data privacy, and data sharing are critical for responsible data management. "],["data-privacy.html", "Chapter 2 Data Privacy 2.1 Privacy vs Security 2.2 PII (personal identifiable information) 2.3 PHI (protected health information) 2.4 PHI is a subset of PII 2.5 PHI Risk 2.6 Cancer research data and PHI 2.7 How to ensure the privacy of this information 2.8 How is HIPAA enforced? 2.9 Summary", " Chapter 2 Data Privacy Cancer research often involves personal health data that requires compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. In this chapter, we will discuss data management strategies to maintain compliance with these important regulations. Cancer research often involves the collection of information about research participants that is personal. There are two categories of such information: personal identifiable information (PII) and protected health information (PHI). Note that these are general definitions and whether something counts as PII or PHI has to be evaluated in a case-by-case basis by an expert such as an Internal Review Board (IRB) member or compliance officer. 2.1 Privacy vs Security So what exactly is privacy? There are a couple of major ways to think about this. The first is keeping other individuals from finding information about others from a legal stand point. In other words, there are legal restrictions like HIPAA to help protect the rights of individuals, by keeping others from accessing information about them. Beyond what is required by law, which may vary depending on what country you perform research in, there are ethical guidelines that define beyond legal ramifications, why someone should protect the privacy of data. In other words, the legal system defines what we have to do, while ethics defines what we should do. Data privacy has a close relationship with data security. Both are concerned with keeping the data from being accessed by those who should not have access. Security is however more concerned with the actual process of protecting the data from unauthorized people, as well as protecting the data from other forms of damage, while privacy is more concerned with who can access the data and use the data how (Bambauer 2013). 2.2 PII (personal identifiable information) PII (personal identifiable information) are aspects of a person that could allow you to identify a person. PII is defined by the US Department of Labor as: “Any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.” PII is also defined by the US General Services Administration as: “Information that can be used to distinguish or trace an individual’s identity, either alone or when combined with other personal or identifying information that is linked or linkable to a specific individual.” Why is this term defined by the Department of Labor and the US general Services Administration? Because the Privacy Act of 1974 (“Privacy Act of 1974” 2022), is a US federal law that governs the “collection, maintenance, use and dissemination” of personal information. US agencies have access to a large amount of PII and must act in accordance with the Privacy Act to protect this data. Examples include (but aren’t limited to): Name Telephone number Address Social security number Age Driver’s licenses Medical record numbers Full face photographs IP addresses Some PII as in the examples above can pose significant risk to individuals if other people were to gain access, such as social security numbers. Other PII such as age does not necessarily pose as much risk unless combined with other information. Thus this information is categorized in two ways as being nonsensitive which is easy to find and poses little risk and sensitive information which is harder to find, poses higher risk and requires more protection. 2.2.1 PII Risk What is the risk of PII getting into the hands of people it shouldn’t? Why was the Protection Act necessary? PII can pose a risk for identity theft, which can have financial, professional, criminal, and personal consequences (DiNardi 2022), as criminals can get loans and credit card in other people’s names, as well as commit crimes under the guise of other people’s identities. This can result in reputation loss and loss of opportunities. A leak of PII can also pose a safety risk, as criminals can identify the likely locations of specific individuals if performing targeted crimes. In addition, a leak of PII might breach patients’ trust in an organization’s ability to keep their data safe and therefore may be less interested in engaging with the organization. 2.3 PHI (protected health information) The U.S. Department of Health &amp; Human Services describes protected health information (PHI) as: …information including demographic data that relates to: the individual’s past, present or future physical or mental health or condition, the provision of health care to the individual, or the past, present, or future payment for the provision of health care to the individual This includes 18 categories: Patient names Geographical elements (such as a street address, city, county, or zip code) Dates related to the health or identity of individuals (including birthdates, date of admission, date of discharge, date of death, or exact age of a patient older than 89) Telephone numbers Fax numbers Email addresses Social security numbers Medical record numbers Health insurance beneficiary numbers Account numbers Certificate/license numbers Vehicle identifiers Device attributes or serial numbers Digital identifiers, such as website URLs IP addresses Biometric elements, including finger, retinal, and voiceprints Full face photographic images Other identifying numbers or codes 2.4 PHI is a subset of PII PHI is really a subset of PII. It is personal identifiable information that relates to or could relate to health. Some PII is always PHI, like health insurance numbers or clinical data such as radiology reports with names or other distinguishing features. Other PII becomes PHI based on context. For example, name and email address aren’t necessarily PHI, unless the are in the context of medical care or research. This could be the case if a patient receives notes from the doctor through email or researchers have a database of participants with email addresses that could be used to distinguish the identity of people in the study. 2.5 PHI Risk PHI poses an additional risk rather than just typical PII. That is because the health information related to PHI, can be used to determine if an individual has a particular condition or health risk and this information could be used against the individual when it comes to employment or insurance. This is particularly an issue if conditions are not known by others or the condition is stigmatizing. 2.6 Cancer research data and PHI Certain genomics data can be used to identify individuals, as well as certain radiology images with particular distinguishing features. With advances in machine learning more of these data types may become identifiable in the future as well. However in general, clinical data and certain genomics data (particularly whole genome sequencing - which is essential a genomic signature) are more likely to be identifiable and therefore pose a higher risk to research participants. So what does this mean for the data you handle? A non-comprehensive list of identifiable and protected information: Clinical information in metadata Genomic sequences Whole genome sequences Exome sequencing Whole transcriptome sequencing Single nucleotide polymorphisms Geneology information What is not protected and generally is safe: Summarized cohort data Data in which individuals have been aggregated together is generally safe. For example, a file that includes an average age calculated across all individuals or a large subset would generally be considered safe. However, this may not always be the case with individuals with very rare conditions. There can also be exceptions to the assumption of safety and/or anonymity when cohort data involves specific groups of people. De-identified data Data in which information about the individuals is removed from the genomic data can be safe to share, but it depends on the genomic data. If the genomic data contains rare variants in which the individuals could be identified or the type of genomic data more readily allows the individuals to be re-identified then the data must be protected (see below on ways in which such data can be shared but with restricted access). It has been shown that certain types of de-identified genomic data can be re-identified due to the availability of genomic data in datasets like 23andMe, where relatives with unique genomic features can be used to identify relatives of individuals in studies. The following articles have more extensive information about the current re-identification risk of different genomic data types: Privacy considerations for sharing genomics data Identifying personal genomes by surname inference Preserving genomic privacy via selective Sharing Impact of HIPAA’s minimum necessary standard on genomic data sharing Genetic information privacy The law and medical privacy The broken promise that undermines human genome research 2.7 How to ensure the privacy of this information Your institution will have guidance regarding how to keep this information private and protected but in general there are 4 main strategies we will summarize here: As few eyes as possible The protected data is seen by the smallest number of individuals possible, all of whom have been properly trained and certified to handle the data. Make sure the data is stored in a place that only these few people who are allowed have access to it. If you aren’t sure who has access to a place – don’t put the data there! Aggressively de-identify the shared data Before results or data are shared or published, it is aggressively de-identified. We will talk more about what this is in the next chapter. If data has been summarized over the cohort and there are no identifiers then it is probably safe to share. Consider a data use agreement Data use agreements (DUA) are not typically required for HIPAA compliance when sharing de-identified data. However, if you are unsure if your data still meets compliance requirements, you have other ethical concerns about sharing your data (which we will discuss in later chapters), consider a using an agreement. Finally if you need to share data that is not fully de-identified, than a data use agreement is required. Data use agreements essential restrict who can access and use the data that you might share, as well as what they may do with the data. Importantly this needs to be agreed upon by an IRB and consented to by the research participants in some manner (more on this to come) before it is in use. See here and here for more information about when you might need a data use agreement. Note that your particular situation and institute may have slightly different rules or restrictions. See here for an example DUA template from the Harvard Catalyst. Be sure to follow the attribution guidelines outlined in the link if you adapt the template for your use. Err on the side of caution! If in doubt if something counts as PHI or PII, consider reaching out to an office at your institute that can help, such as possibly an Internal Review Board (IRB), a research administration office, or a HIPAA compliance office. If you plan to share your data somewhere and you are unsure whether a database or repository is secure and HIPAA compliant, ask those who manage that database or repository! 2.8 How is HIPAA enforced? The Office for Civil Right (OCR) of the United States Department of Health and Human Services is in charge of enforcing HIPAA compliance. If you feel that someone is using or sharing data that is in violation of HIPAA compliance, you can file a complaint online using the OCR compliant portal. Typically you should see if you can get the violation to be resolved through local means by contacting research administrators or management. However, you could submit to the OCR. Note that complaints should be filed within 180 days of the violation. If the OCR determines that a covered entity (the individuals or institutes who are required to follow HIPAA compliance regulations), then the OCR will follow up to ensure that the entity complies, takes corrective action, or agrees to a settlement. If compliance is not resolved, then the covered entity may have to pay fines. Currently if an individual is not aware of a violation the fine can be quite small, but if it is a repeated issue of willful neglect, they can be fined on the order of $50,000! If the entity committed the violation for malicious reasons for personal gain, they can face much higher fines, up to $250,000 and may face jail time of up to 10 years (eLearning 2018). If it is deemed that a breach has occurred, the organization responsible for the breach is required to let affected individuals know. See here for more information. 2.8.1 Common Violations Common violations of HIPAA taken from eLearning (2018) are: A lack of encryption If your email or data transfer is intercepted it is important to keep your data safe! We will talk more about how to do this in the next chapter. Computer hacking or phishing If your computer gets hacked by hackers through a phishing email or otherwise, they could sell the data to third party organizations who could profit off of the information. The data security practices that we will describe in the next chapter will help avoid this. Unauthorized Access Allowing or accidentally allowing fellow lab mates who are not authorized to access the data is a violation of HIPAA. Generally this does not result in harm, but occasionally this can result in other neglectful or malicious practice that result in larger disclosures of PHI. Furthermore leaving your laptop open to PHI data in public or even at home can pose a risk from people who walk by. Loss or Theft of Devices If your laptop are external storage device is stolen, data files with PHI can easily be obtained by whoever finds them next. Again the measures in the next chapter will help to avoid this potential issue. Improper Disposal of data or devices Sometimes there are remnants of your data still on your device! Unsecured access to data Accessing your data form an unsecured WIFI network can also make the data vulnerable. See here for more information about HIPAA and research. In the next chapter, we will talk about measures that you can do to avoid these violations. 2.9 Summary In summary, we covered the following concepts in this chapter: Cancer research often involves personal health data, requiring compliance with HIPAA regulations. There are two categories of personal information: Personal Identifiable Information (PII) and Protected Health Information (PHI). PII is information that potentially allows for the identification of an individual PHI is a subset of PII that pertains to an individual’s health, including medical records, health insurance numbers, etc. PII and PHI pose risks, including identity theft, safety risks, and potential discrimination in employment or insurance. Certain genomic and clinical data may pose higher risks for identification, requiring more protection in cancer research. Strategies to ensure data privacy and compliance with HIPAA: limit access, aggressive de-identification, data use agreements, and seeking guidance. HIPAA is enforced by the Office for Civil Rights, with penalties for violations, including fines and corrective actions. Common violations include lack of encryption, hacking, unauthorized access, loss/theft of devices, improper disposal, and unsecured access to data. References "],["data-security.html", "Chapter 3 Data Security 3.1 Data Security Strategies 3.2 Data access 3.3 Data masking 3.4 Data erasure 3.5 Data resiliency 3.6 Email security 3.7 Summary", " Chapter 3 Data Security In this chapter we will discuss best practices for keeping your data safe and secure. Data security involves protecting your data from human error, as well as from cybercriminals. Precious data needs to be protected from being corrupted (rendered unusable), being deleted, being misused, and being leaked to the public when the data is sensitive or private (as is the case with much of our clinical data)(“What Is Data Security? Data Security Definition and Overview” n.d.). 3.1 Data Security Strategies There a several major strategies to achieve data security. We will cover the following 4 major categories, in part according to “What Is Data Security? Data Security Definition and Overview” (n.d.). 3.2 Data access 3.2.1 Authentication Authentication is the process of verifying identification. This goes both ways when a user attempts to use a server. Servers will check that a user (client) is who they claim to be and that they are authorized to access the information on the server. The user’s computer (client) will also check that the server is what it claims to be. Typically users will use a user name and password for the server to confirm identity, however your computer often checks a certificate from the server to confirm it’s identity (“Understanding Authentication, Authorization, and Encryption : TechWeb : Boston University” n.d.). 3.2.2 Authorization Authorization is the process of verifying that a client has permission to access a file or computing resource in a particular way, which leads us to permissions (“Understanding Authentication, Authorization, and Encryption : TechWeb : Boston University” n.d.). 3.2.3 Permissions If you use the command line, you may have noticed that files are sometimes listed with letters like so: These letters indicate the users ability to perform tasks for each file called file system permissions. In unix or unix-like systems, there are three types of users: Owner (sometimes called user) - This is the person that created the file by default. Group users - Groups can be created to more easily give the correct permissions for various files for a similar group of users. Other users - This is any other user who might come into access with the file. There are generally 3 types of permissions: read (r) - the user can read the contents of the file, or the user can just read file names in the case of a directory (or folder) write (w) - the user can modify the file, or the user can create or delete files in the case of a directory (or folder) execute (x) - the user can execute a file - this allows the operating system to run the file, for example a user could run a script, or the user can search among files in the case of a directory (or folder) Values for permission follow a standard form: The first value indicates information about the file type, if it is a regular file it is often just a -. This does not indicate any information about permissions. If it is a link to a file located somewhere else it is indicated with an l. The next 3 values indicate what the owner of the file can do. They are always listed in the order of read, write, and execute permissions. The next three values indicate what the group members can do (again with the order listed above). the final three values indicate what other users can do (again with the order listed above) (“File-System Permissions” 2021). Sometimes you will see an @ or + symbol at the end, which indicates that there are additional attributes about the file that you can use a command like ls-al@ filename to get the information, or additional security information about the file, which you can obtain using a command like ls -le filename or getfacl filename. The command that you should use depends on your operating system. In the above image we see -rwxr-xr-x for one of the files. The - at the beginning indicates that the file is a regular file, the next 3 values of rwx indicate that the owner has read, write, and execute permissions, the r-x indicates that the group has read and execute permissions but not write, and the last r-x indicate that the other users also have only read and execute permissions. There are commands to modify file permissions. If you are using a Unix-like system, you can modify permissions with the chmod command, which stands for “change mode” (“Manage File Permissions on Unix-Like Systems” n.d.). 3.2.3.1 The principal of least privilege The principal of least privilege (PoLP) also called the principal of least authority specifies that users should only have access to the data or resources necessary to complete a task. If someone does not need access to perform work, they should not be given access. Furthermore, if someone is given access to perform a task and they complete that task, access should then be removed from that person. Additionally, if someone only needs to read a file, then they do not need access to modify the file. In this case a user can have read permissions but not write permissions. Ultimately this type of management leads to the least number of users having access to read or alter protected or sensitive data at a given time (“Least Privilege CISA” n.d.; “Wikipedia: Principle of Least Privilege” 2021). As an example of this in practice: If a postdoc is working on a project with sensitive data, and the PI does not need to see the raw data to collaborate on the project, then the PI should not have access to the data. 3.2.4 WiFi Make sure you also only access secure WiFi networks. One way to ensure this is is to avoid using public WiFi networks. If you must use a public WiFi network, consider using a virtual private network (VPN) for added security. Here is an article about different VPN options (Gilbertson 2021). 3.2.5 Passwords Passwords need to be effective. This means they should be hard to guess by people who should not have them. Some people suggest using sentences that are easy for you to remember, you could consider a line of lyrics from song or poem that you like, or maybe a movie. Modify part of it to include symbols and numbers (“Guidelines for Strong Passwords · Information Technology Services · Lafayette College” n.d.). Don’t share your password and keep it safe! We highly suggest you consider a password manager to keep your passwords extra safe and secure. If you have a Mac, you could consider storing it in your Keychain, alternatively if you have a different type of computer or don’t like the Mac Keychain, consider Dashlane or other password manger services. Luckily both of these options do not come at any extra cost and can be helpful for storing all the passwords we use regularly safely. These are especially good options if your password is difficult for you to remember. 3.2.6 Cached data In addition to storing data where you might expect, your computer also sometimes stores your data temporarily using something called a cache (“Cache (Computing)” 2022). A cache is basically a small storage of your recent use of data. This allows you to access the data that you have been using more quickly next time you want to do something with it. This process is used by the computer processor or CPU called a CPU cache, as well as software, which we will call a software cache. If you are working with data stored on your local computer, then a subset of short-term memory will be used for the cache, rather than requiring the user to access the data from where it is typically stored much further away in long-term memory. This is more efficient because it is located closer to processor and thus allows for requests on that data to happen faster (“Memory Caching” n.d.). This general process is also used for accessing data on servers or websites. For servers, a cache is often stored on the server side, while often for websites, a cache is stored locally on the user’s computer in the long-term memory so that when users access the site again, access will be faster. Web browser caches can store your browsing history, cookies (a unique ID for the website to identify the user), as well as sensitive data. Although this makes your work faster, caching poses some security risk. Caching provides additional locations where hackers could access your sensitive data. Furthermore, the data in caches are often not encrypted, making such data more vulnerable. One way to avoid the security risk associated with your cached data, is clearing your caches (“What Is a Cache? UpGuard” n.d.). In the case of a CPU or software cache, security concerns are heightened if your laptop gets stolen, lost, or if you decide to sell your laptop. The easiest way to clear a CPU or software cache is to simply shutdown your computer. If you have taken our computing course, you will learn that data stored in short-term memory (like RAM) requires electricity, and it will disappear when your computer is no longer connected to power (“What Is a Cache? UpGuard” n.d.). Note that it can take a few minutes for such memory to disappear. See here for instructions on how to clear browser caches. It’s a good idea to clear your browser cache relatively often (ideally before anytime you take your computer out where it could be stolen or lost), and possibly more often if you access sensitive data on your computer regularly. 3.2.7 External drives There are a few major reasons why external drives, especially flash and USB drives (also called thumb drives) pose a security risk. they are small and especially portable and thus easily taken or lost often these drives can contain malware that gets installed on your computer when you plug it into your computer the memory sometimes fails and gets damaged or corrupted There are several strategies you can take to avoid these issues, if you must use such a drive Durken (2021): Never use a drive that you find randomly. Ideally only use drives you get from a reputable manufacturer. Even drives at conferences could pose a security risk. Use drives that have encryption (either buy drives that already have it, which is recommended or add encryption to your drive). Disable AutoRun software which allows drives to automatically be opened. See here for how to do this. Note that this is mostly an issue for Windows computers. Remove the drive when not in use (it improves the life of the drive and therefore protects data on the drive from disappearing or getting damaged). Keep all your software up to date to get important security updates. Editing files directly on the drive can limit the life span of the drive, instead simply copy paste files back and forth from your computer. If you really need to use a drive that is questionable, check out this article for practices to more safely open such a drive. 3.3 Data masking Data masking refers to several several methods for protecting sensitive data. One example you might be familiar with is encryption. Encryption is a relatively complex method of data masking. There are other methods for obscuring parts of the data that are less complex, such as simply removing pieces of information, showing only parts of data values or shuffling the data. 3.3.1 De-identification Data de-identification is the process of removing any values that could be used to identify an individual. In other words, it is the process of obscuring the identity of the individuals who have data values within a data set. Re-identification is the process of determining the identity of such individuals based on data (“De-Identification” 2022). Typically an ID number is assigned to each individual to allow for re-identification. The key for these ID numbers and the original identifier information should be stored in a safe manner with a limited number of individuals with access. The ID number should not be created by using any identifiable information (for example birthday -15*32 followed by the first digit of Zip code). In some cases people use what is called a cryptographic hash function which is a condensed representation of an individual’s data (including identifiable information) to a standard ID length. Such functions are HIPAA compliant under special circumstances, such as restricted access because they are very difficult to decipher. These hash function IDs have an advantage of allowing for the same individual across different datasets to be tracked, which can be helpful for longitudinal studies (“HIPAA Privacy Regulations: Other Requirements Relating to Uses and Disclosures of Protected Health Information: Re-Identification - § 164.514(c)” n.d.). Hash function IDs are not intended for re-identification. See here for additional information about HIPAA de-identification regulations. While somewhat useful for protecting the identity of those included in data, de-identification alone is not necessarily robust to newer re-identification methods. Thus other security, privacy, and protection methods are necessary. The HIPAA Privacy Rule specifies two methods or standards for de-identification: Safe Harbor and Expert Determination. While it is possible to de-identify your data yourself, unless you are an expert, we suggest that you seek out an expert to help you instead. There are nuances regarding privacy that you may miss without ample experience. 3.3.1.1 Safe harbor The safe harbor method is the more extreme of the two and results in more loss of data. A list of 18 standard identifiers must be removed from the data. These are the same 18 PHI categories of identifiers that we described in the last chapter, including name, IP addresses, etc (Rights (OCR) 2012). Additional rules are required as well, see here. For example zip codes may remain in the data if they are shortened to the initial 3 digits and meet certain population threshold criteria. Dates can only remain at the year level, with extra protection for individuals over 89 years of age. However other abbreviations, such as name initials are not permitted. 3.3.1.2 Expert Determination The Expert Determination method attempts to preserve more of the original data, and data is removed based on the risk of re-identification. As the name describes, an expert evaluates the risk for re-identification of the various values within the data including what the risk might be with if the data were to be combined with other data sources. Ultimately data is removed until there is only a very small risk of someone being able to re-identify the individuals with data values among the dataset. The expert records what methods they used and the analysis that they did to determine the risk. There is no standardized single way to mitigate risk, but again a set of methods. These typically involve slight modifications to the data called perturbation, coding certain values called generalization or removal of data called suppression. These can occur at the feature level, such as all ages, at the individual level, such as removal of one individual, or at the value level, such as one data value for one individual. An example of perturbation could be the modification of age by up to 5 years. An example of generalization would be that all individuals over a certain age could be given the same age group value. As example of suppression at the feature level, one could remove all names. As example of suppression at the individual level, in some cases there are individuals who are themselves very distinguishable due to some extreme attribute about themselves and may need to be removed due to the uniquely high risk of identification. If there is just one aspect about these individuals, than that value for the individual can be removed instead of removing the individual from the data entirely. 3.3.1.3 Limitations In some cases removing protected information can result in data loss that could impede research initiatives. However, generally the limitations of de-identification are the risks that they pose for re-identification. For the Safe Harbor method, de-identification based on 18 PHI categories does not necessarily protect from re-identification, particularly given the ubiquity of large data sets. Even though it is the more stringent method, Safe Harbor has privacy limitations. There are other aspects about the data that could be or could become identifiable. For example, if an individual has a very unique occupation (such as state senator) or combination of certain remaining characteristics, they could become identifiable. Another example is if an individual was a very unique clinical case. For the Expert Determination method, the determination of risk of re-identification is subjective. There is no specific degree requirement or training to be considered an “expert”, however typically they are individuals with a statistical background and people who have had experience in making risk determinations. There is also no specific threshold of what constitutes a very small risk. This is determined by the expert. Additionally, there is no required length of time that a determination expires. It is highly suggested that additional evaluations occur over time as the risks change as technology changes and as the uniqueness of the individuals within the dataset evolves. For example if there are much fewer people over a certain age still living in a sparsely populated area, they could become identifiable. There is also no standard single method to assess re-identification risk. Instead there are a set of methods that follow major principles of risk assessment. They are based on the ability to reproduce the data values for the individual, the availability of identifiable data values from other sources, and the uniqueness or distinguishability of the data values. However, whatever methods are used, the expert is required to document this and make it available to the OCR if they request it. 3.3.2 Encryption Encryption is one of the most well-known methods for keeping data safe. It is used as a last method in case unauthorized users can access to data, and it is also used to protect data when transferring it. The process involves encoding or scrambling the data in a nonrandom format (we call this form of the data encrypted) that given the right instructions, a computer can reformat into the original form in a process called decryption. The instructions called the key are kept safe like a password and depending on the type of encryption, it can require quite a lot of computational power to decrypt the data. This protects the data because if a cybercriminal accesses the data or if the data is somehow made public when it shouldn’t be, it will not be easily interpreted (“What Is Data Encryption?” 2018). There are different methods for encrypting data. One common method is called asymmetric, which involves two keys, a public key and a private key (“What Is Encryption? Data Encryption Defined” n.d.). This method is also sometimes simply called public key. Users can get access to the public key to allow them to encrypt the data, while the private key remains private and is used to decrypt the data. (“What Is Encryption? Data Encryption Defined” n.d.). Symmetric cryptography on the other hand uses one key for encryption and decryption. In systems that use this type of encryption, pairs of users will often be given their own key. The advantage of this system is that decryption is a bit faster, the keys are smaller, and it is generally less expensive to implement. If someone gains access to the key, however they can decrypt data or messages, and encrypt data or messages and appear as if they are the person that owns that key. So often the keys themselves are encrypted (“What Is Encryption? Data Encryption Defined” n.d.). Since symmetric decryption is faster, it is often used for transferring data or for large datasets. Common symmetric algorithms are AES-128, AES-192, and AES-256 (Daniel, n.d.). Asymmetric encryption is regarded to be more secure, common algorithms included RSA and DSA, and several PKCS standards (Daniel, n.d.). These algorithms involve mathematical operations to encrypt the data. See this video for a simplified explanation and a suggestion for a video series if you want to learn more about how these encryption algorithms work: 3.3.3 Security Protocols Data is masked and kept secure by different protocols. Two that are quite common and sometimes get confused are SSH (Secure Shell) and SSL (Socket Layer). Both create secure connections and encrypt data to keep it safe. What differs between these protocols is the destination of the connections being made. Where SSL is for moving data from a website to a server, SSH is for making connections to execute commands on remote computers. 3.3.3.1 SSL Socket Layer This is what the s in https is for. This indicates that data from the server is using a form of encryption and that the website server has a special SSL certificate from a trusted certificate authority that verified that the server can be trusted. There is actually a lot happening behind the scenes! According to “What Is SSL (Secure Sockets Layer)? What Is an SSL Certificate? DigiCert” (n.d.) this involves the following steps: A web browser (like chrome) connects to a web server (website) that is secured with SSL (it starts with https instead of http). The browser asks the server to identify itself. The server sends a copy of the SSL certificate and the public key. The browser checks the certificate to make sure that it comes from a trusted certificate authority and that the certificate has not expired or been revoked. The browser also checks that the names on the certificate and the website match. It then sends the server an encrypted session key based on the public key. The server then decrypts the session key using the private key and sends an encrypted acknowledgment to start an encrypted session. The server and browser can now transmit encrypted data back and forth. Note that newer versions of this process are called Transport Layer Security (TLS). 3.3.3.2 SSH Secure Shell SSH (secure shell) is also a process for securely transmitting files from one computer to another using encryption. However, in this case it can be done using a command line interface instead of a browser. SSH does not involve certificates. SSH is used for communicating to remote computers which is why it is commonly used at institutions to get access to high powered computing clusters. SSH works like this(“SSH Protocol – Secure Remote Login and File Transfer,” n.d.): The user initiates a connection by submitting a command to contact the server The server returns a key With this key a channel between the user’s computer and the remote computer cluster can be opened. Now the user can login to the remote computer’s operating system. For more about SSH and SSL: SSH and SSL exploring the similarities and differences SSL vs SSH simplified 3.4 Data erasure Erasing data (data erasure) is not as easy as it may seem. It turns out that when you delete a file (even after emptying the trash), it isn’t as “deleted” as you might think. This is because when a file is deleted, the data for that file actually stays on the storage hardware, and it’s simply the computer’s ability to find the data that is hindered. However, there is software that can help people recover data on storage hardware. This can be a great security issue, as sensitive data can remain on people’s hardware when they get a new computer or stop using particular data on a server. One method to ensure that the deleted data is really eliminated is to physically destroy the hardware that it was stored on. Alternatively, there are methods using software. Software methods are often optimal because the hardware can be reused without allowing future users potential access to your data. As you might imagine, software methods are preferred for erasing data on shared computing resources like servers (Tilly Holland 2020). These methods erase the data by overwriting the data with random digital information (“Data Erasure” 2021). 3.5 Data resiliency If you are working with precious data directly on your laptop, in case something happens to a computer, it’s a good idea to think about storing the data with multiple locations in case something happens to a computer The traditional 3-2-1 backup rule recommends that you keep at least three copies of your data, two different copies stored on separate formats and at least one additional copy at an offsite location.(Durken 2021) For example, you might think about having one copy on your password protected laptop, one encrypted and zipped copy on your password protected external drive at home, and another encrypted copy on your password protected external drive in your office. However, in the age of cloud computing and servers, this rule needs to be updated a bit. It has been suggested that if you make use of remote computing options, instead there should be: 3 additional copies of the data in addition to the original 2 different places on a server or 2 regions in a cloud (where possible) where the data is stored 1 copy of the data should be stored closer to where it was originally created just in case something happens with the server or cloud that you are using (Posey 2021). 3.6 Email security Email can also pose a risk for our data. This can be due to the security risks of actually emailing our data to a colleague or issues related to having our computer hacked by someone through phishing emails. 3.6.1 Email Encryption If possible we would suggest that you avoid sharing data through email, and instead share data through other means, such as a shared computing platform, especially if you have data that requires HIPAA compliance and you have identified a platform to work on that is HIPAA compliant. Please see our computing course for more information about this. However we understand that there are situations where you might need to email data. If your data has any PHI or PII, we suggest that you only send such emails using a secure network and using an encrypted email. How do you do this? If you use Outlook, there are options to do so currently under the “Draft” menu, by selecting the “Encrypt” option. Gmail also supports encrypted emails. Be sure to Google how if this course becomes out-of-date. Your institute may have special ways of encrypting emails, so check with your local IT department. 3.6.2 Phishing You likely have heard from your IT department at your institute about being careful about phishing. If you are working with PII or PHI, then you need to be extra vigilant. Phishing emails are emails in which someone tries to get you to click on a link or to respond with credential information in order to hack your computer or gain access to sensitive data. Spear phishing occurs when someone poses as an individual, often someone who is more senior at the institution (this is specifically called whaling), to gain information from others (“Phishing” 2022). Criminals are getting more sophisticated in their attempts, making it more difficult to avoid. Here are some things to look out for: Be very weary about emails from unrecognized senders! You can look up the sender to verify if the person is who you expect. If the email looks suspicious but still potentially real, you could contact the individual on linked-in or elsewhere to verify if the person actually contacted you. If their email does not match their name this is also extra suspicious. For example, if you get an email signed by George and the email is Peter125@network.org, this could very well be a phishing attempt. Check the sender’s email. Make sure that the email address from the sender looks like what you recognize. If you know the sender and they send you an email with an unusual email address, be careful. You can send them an email to their typical address to verify if it is really them. For example if your boss Karen, sends you an email from kit345@TSU.edu or titan@hotmail.com and typically her email is karenw3@TSU.edu, you should be suspicious. Watch out for links. Avoid clicking on links in emails as much as possible! If you know that your colleague is sending you a link and you see it right away, that is probably trustworthy, but if your admin sends you a link out of the blue, you should be careful. If you must click a link, first make sure that the link looks like what you would expect. Second, send a follow-up communication using another method (phone, slack, different email address), or email a different colleague that you know works with the individual to make sure the individual actually sent the link instead of a hacker. Make sure that you don’t get the phone number or other information to validate if the individual really sent you the email from the suspicious email itself. An example of these types of phishing methods is if your colleague hasn’t told you that he is sending a Google doc link and you receive an email from him with a link, then do not click it before verifying that the person really intended to send it. Yet another example is if an administrator sends you a link for you to update your password. Typically they will instead have you go to whatever portal you need to go to manually on your own to update your password. Keep in mind that phishing criminals can make the emails look very legitimate! Here is a real example of a such a phishing email from California State University Northridge: From: xxxxx, xxxxx&lt; &gt; Date: Tue, May 25, 2021 at 3:01 PM Subject: Action Required | Returning to Campus Guide for Employees Training Dear Colleague, Our records show that you have not completed the Returning to Campus Guide for Employees Training. This training is required to ensure CSUN is in compliance with California OSHA law which mandates that all employees be trained on COVID safety. Please complete this important safety training video ASAP. If you are having issues accessing the course, let me know. Here’s a shortcut to access the training, (Link has been removed) from this link, you’ll select “Northridge” from the drop down and then sign-in with your CSUN credentials. Then select the “Start” button to view the training. Be especially careful of links from two similar emails. Clone phishing is a sophisticated method in which a legitimate email may be maliciously changed and resent to recipients claiming that there was a necessary change. Be especially careful in such situations where you receive nearly duplicate emails with links and reach out to those who sent it to make sure that a repeated email was really from them (“Phishing” 2022). Be careful about attachments! Similar to links, if you receive an email with an attachment that you do not expect you should be careful and not open them unless you verify by other means that the sender really meant to send you an attachment. Don’t give your credentials in emails! Be careful giving personal information. If someone asks for your credentials or personal information, you should be especially concerned. Generally you will not need to share your password with anyone. Typically administrators will have you log in to whatever platform or system you are working in and proceed from there or they will reset your password. If someone asks for your password or username, it is highly likely a phishing attempt, even if it seems like it is coming from a verified source. An example of this type of phishing is if the chair of your department suddenly (out of context) asks you for your personal phone number. Before sending this information, contact the chair by other means to make sure this request is legitimate. Watch out for urgent situations. Often phishers will create a false urgent situation to trick you into clicking a link or giving information. Here is a real example of a such a phishing email from California State University Northridge: Hello there! My name is Sarah. Your website or a website that your company hosts is infringing on a copyrighted images owned by myself. Check out this doc with the hyperlinks to my images you utilized at (URL removed) and my previous publication to find the evidence of my copyrights. Download it now and check this out for yourself: (URL removed) I believe you’ve intentionally infringed my rights under 17 U.S.C. Sec. 101 et seq. and could possibly be liable for statutory damage as high as $150,000 as set forth in Section 504 (c)(2) of the Digital millennium copyright act(DMCA) therein. This message is official notification. I demand the elimination of the infringing materials referenced above. Take note as a service provider, the Dmca demands you, to remove and/or deactivate access to the copyrighted materials upon receipt of this notification letter. If you do not cease the utilization of the previously mentioned copyrighted content a law suit will likely be initiated against you. I do have a strong belief that utilization of the copyrighted materials referenced above as presumably infringing is not approved by the copyright proprietor, its agent, as well as legislation. I swear, under penalty of perjury, that the information in this letter is accurate and that I am the copyright proprietor or am certified to act on behalf of the proprietor of an exclusive and legal right that is presumably violated. Sincerely, Sarah Olson 07/21/2021 Additionally, be careful logging into websites as well. Make sure that you are indeed at the link you expect - one way to avoid this is to make a bookmark of the sites that you log in to. See here for recommendations on what to look out for from the HIPAA journal. 3.7 Summary In summary, we covered issues related to data security in this chapter. We presented the following concepts: Authentication is the process of verifying the identity of users and servers in a communication. Users provide their credentials (username and password), while servers present certificates to confirm their identity. Authorization is the process of ensuring that someone has permission to access a file or computing resource in a particular way. In Unix-like systems, files have permissions for three types of users: owner, group users, and other users. These permissions include read, write, and execute privileges, and they are represented by letters (r, w, x) in file listings. Principal of Least Privilege: This principle states that users should only have access to data or resources necessary to complete their tasks. Unnecessary access should be avoided, and privileges should be revoked when no longer needed. It is crucial to use secure WiFi networks and avoid public ones whenever possible. If necessary, use a Virtual Private Network (VPN) to enhance security while using public networks. Strong passwords should be used, preferably in the form of sentences with symbols and numbers. Password managers like Keychain, Dashlane, or other services can help securely store passwords. Computers use caching to store recent data for faster access. Clearing caches regularly is essential to avoid security risks and potential exposure of sensitive data. USB drives can pose security risks due to portability, malware, and memory issues. It’s best to use reputable drives, enable encryption, disable AutoRun software, and remove the drive when not in use. Data masking obscures sensitive data to protect privacy. De-identification and encryption are common methods. De-identification removes identifiers, usually using the Safe Harbor or Expert Determination method. Encryption encodes data to prevent unauthorized access. Encryption involves encoding data with keys to protect it from unauthorized access. Asymmetric encryption uses two keys (public and private), while symmetric encryption uses one key for both encryption and decryption. SSL (Secure Socket Layer) and SSH (Secure Shell) are protocols that establish secure connections and encrypt data. SSL is commonly used for websites, while SSH is used to connect to remote computers. When files are deleted, the data remains on storage hardware, posing a security risk. Software-based data erasure methods overwrite the data, ensuring it cannot be recovered, allowing hardware to be reused securely. To ensure data resiliency, follow the 3-2-1 backup rule, which suggests that you have at least 3 copies, using 2 formats, and at least one different location. For those using shared computing resources, this should be updated a bit. Avoid sharing sensitive data through email whenever possible.Use shared computing platforms, especially if HIPAA compliance is needed. When emailing PHI or PII, use a secure network and encrypted email. Be cautious of phishing attempts: Beware of emails from unrecognized senders. Check the sender’s email address for legitimacy. Be cautious of links and verify with the sender through other means. Be wary of similar-looking emails or cloned messages. Exercise caution with email attachments from unknown sources. Never give personal credentials or sensitive information via email. Watch out for urgent and alarming requests. References "],["data-sharing.html", "Chapter 4 Data Sharing 4.1 Data sharing is important! 4.2 Benefits of data sharing 4.3 Data repositories 4.4 Data Submission tips 4.5 Health care data sharing tools 4.6 Summary", " Chapter 4 Data Sharing In this chapter we will discuss the importance of data sharing, best practices for data sharing, places where you can store your data, as well as methods to share data in contexts in which you might have thought sharing was not possible! 4.1 Data sharing is important! Sharing data is critical for optimizing the advancement of scientific understanding. Now that labs all over the world are producing massive amounts of data, there are many discoveries that can be made by just using this existing data. This is so important, that starting in January, 2023 the NIH will require specific sharing practices for data management and sharing. See the announcement here. See this course for more information about how to comply with this policy. Note that many institutes and funding agencies or mechanisms have requirements about how your data can be shared. Typically data sharing of protected data also requires Institutional Review Board (IRB) approval before the study is conducted. Ensure that you are following those requirements before you share your data. There’s so many excellent reasons to put your data in a repository whether or not a journal requires it: Sharing your data… Makes your project more transparent and thus more likely to be trusted and cited. In fact one study found that articles with links to the data used (in a repository) were cited more than articles without such information or other forms of data sharing (Colavizza et al. 2020). Helps your relieve your own workload so your email inbox isn’t loaded by requests you probably don’t have time to respond to. Allows others to gain even more insights from your data which shows funders that your data will be used to its maximum potential. It also provides more opportunities for others to replicate your results, which could help advance not only your career, but our understanding of science and medicine. 4.2 Benefits of data sharing In addition to these benefits to yourself, data sharing has other far reaching benefits. It can help support faster advances in science and medicine, by reducing the need to collect new data, which reduces costs, time and effort, including the effort and burden that is required to collect data on or from patients. It also helps support researchers at institutes that do not have as many resources to collect data. Ultimately it can also therefore help patients benefit from research faster, as faster advances can be made through more efficient research. 4.3 Data repositories The best way to share your data is by putting it somewhere that others can download it (and it can be kept private when necessary). There’s many repositories out there that handle this for you. Below are some of the standard repositories for data you should consider. For a longer list of repositories, we also advise consulting this Guide on data repositories](https://www.nature.com/sdata/policies/repositories) published by Nature. 4.3.1 Genomic Data Repositories National Center for Biotechnology Information (NCBI) For microarray: GEO Gene Expression Omnibus (GEO) For RNA-seq: SRA (Sequencing Read Archive) European Molecular Biology Laboratory-European Bioinformatics Institute (EMBL-EBI) International Nucleotide Sequence Databases—DNA Data Bank of Japan (DDBJ) 4.3.2 Imaging data repositories Imaging data resource Cancer imaging archive 4.3.3 Repositories for journal articles For manuscripts or large datasets that are of atypical format, using one of these repositories is a good idea. The journal you submit to may have a recommendation of one over another. If not, you might end up having a preference. CyVerse Data Commons Repository Data Dryad FigShare Zenodo GitHub 4.3.4 Small datasets Data sets that are small or have an atypical format can be published as supplementary files as a part of a manuscript. 4.4 Data Submission tips Uploading a dataset to a data repository is a great step toward sharing your data! But, the dataset uploaded is unclear and unusable it might as well not been uploaded in the first place. Keep in mind that although you may understand the ins and outs of your dataset and project, its likely that others who look at your data will not understand your notation. To make your data truly shared, you need to take the time to make sure it is well-organized and well-described! There are two files you should make sure to include to help describe and organize your data project: A main README file that orients others to what is included in your data. A metadata file that describes what data are included, how they are connected. Standards for genomic metadata 4.4.1 Use consistent and clear names Make sure that sample and data IDs used are consistent across the project - make sure to include a metadata file that describes your samples in a way that is clear to those who might not have any prior knowledge of the project. Sample and data IDs should keep with standard formatting otherwise known in the field. Features names should avoid using genomic coordinates as these may change with new genome versions. 4.4.2 Make your project reproducible Reproducible projects are able to be re-run by others to obtain the same results. The main requirements for a reproducible project are: The data can be freely obtained from a repository (this maybe summarized data for the purposes of data privacy). The code can be freely obtained from GitHub (or another similar repository). The software versions used to obtain the results are made clear by documentation or providing a Docker container (more advanced option). The code and data are well described and organized with a system that is consistent. Check out our introductory and advanced courses about reproducibility for more information. 4.4.3 Have someone else review your code and data! The best way to find out if your data are useable by others is to have someone else look it over! There are so many little details that go into your data and projects. Those details can easily lead to typos and errors upon data submission and also can lead to confusion when others (or your future self) are attempting to use that data.The best way to test if your data project is usable is to have someone else (who has not prepared the data) is able to make sense of it. For more details on how to make data and code reproducible tips, see our Intro to Reproducibility course. 4.5 Health care data sharing tools 4.5.1 REDCap (Research Electronic Data Capture) REDCap is a very widely used browser-based software application for managing surveys and databases. It is very often used for clinical data. In fact, it is so widely used that there is a conference dedicated to it. REDCap is a platform that allows for multi-institutional work and is compliant with multiple regulations including HIPAA, 21 CFR Part 11 (FDA data), FISMA (government data), and GDPR (data for the European Union). It was developed by a team at Vanderbilt University in 2004. It is not open-source, however it is free to use for non-commercial research (“REDCap” 2022). You can find out more about how to use REDCap at the REDCap website which includes instructional videos and other resources. There are several things to keep in mind when using REDCap to ensure that data privacy and security are protected. Roles REDCap allows for various roles to be established for users on a project. Thus access to certain data and tasks can be restricted to certain individuals. As described previously, according to the Principle of Least Privilege, it is a good idea to restrict access to the smallest number of individuals necessary. You can modify roles using the User Rights menu. This will first show you who has what role on the project and their rights. You can click on an individual role to modify it. Roles should be verified by your institutional review board (IRB) before beginning a study. Changes to roles should also be reviewed by your IRB. Reports Reports that are exported can be customized to only show data that should be shared with the individual that you plan to share with. Please see the section on de-identification to better understand what data you might want to be restrictive about sharing. Again, the way you intend to share your data should be reviewed by your IRB before you begin your study. For example, you might remove the dates from the following report: Auditing REDCap keeps track of all data modifications, as well as data exports or report generations, in addition to keeping track of who performs those actions. This can be helpful for checking what has happened and when, in case anything happens that is unexpected or unintended. This is also great from a reproducibility or transparency standpoint - you have a record of any modifications to the data. This information can be obtained from the logging menu. Keep instruments short If your instruments are too long, this can result in accidentally sharing data that you don’t intend to, simply because you have more data to sift through. This also makes it easier to generate reports only on specific data that you would like to share. Data can be locked You can protect your data from accidentally being modified by locking specific data. Furthermore, at later stages of the project the data can no longer be modified. Keep in mind that your institution likely has their own guidelines for how to use REDCap should you decide to use it. Also remember to verify what you plan to do with your institutional review board (IRB) before you begin the study. 4.6 Summary In summary, in this chapter we covered the following concepts: Data sharing is important for advancing scientific understanding, transparency, and maximizing the value of your data. There are many data repositories where you can store and share your data, including general repositories like Data Dryad and FigShare, and repositories specific to certain data types like genomics or imaging data. When sharing data, be sure to organize and document your data well with things like a README file, consistent naming conventions, and metadata. Follow reproducibility practices whenever possible. Tools like REDCap can help manage clinical data while ensuring security, privacy, and reproducibility through features like role-based access controls, data auditing, and locking data after collection. Checking with your IRB first before sharing data, sharing code, or using new tools can help ensure that data is shared and accessed responsibly. Ideally such plans should be reviewed by your IRB before you begin a study. It is often possible to safely publicly share the code used to analyze protected data, as long as you don’t reveal aspects of the data in the code. Your local IRB may be able to help you learn how to do so. References "],["data-ethics.html", "Chapter 5 Data Ethics 5.1 What is data ethics? 5.2 After Considerations 5.3 Data ethics history 5.4 Principles of Bioethics 5.5 Ethical Principles for Data 5.6 Concept of Consent 5.7 Medical Ethics Timeline 5.8 Causes of research misconduct 5.9 Consequences of research misconduct 5.10 Misconduct prevention 5.11 Summary", " Chapter 5 Data Ethics Now that we have covered the basics of data management, we will take a moment to consider and reflect on the implications of our use and sharing of data. 5.1 What is data ethics? Data ethics involves the consideration of: data collection data security data privacy data maintenance data sharing It also involves mindfulness about how our research can ultimately impact (or not impact as the case may be for research that lacks inclusivity and equity) research participants and other individuals. Importantly, we do not yet have established societal norms or protocols for every aspect of medical research, particularly with respect to new types of data and new technologies, and many topics are still under debate especially when it comes to cutting edge research. However, general principles of ethics can be helpful and involve practices for research integrity, consideration for social justice, and transparency. Health care and research ethics can also be helpful in evaluating practices for data management and use. 5.1.1 Before and after research Data ethics requires thoughtfulness both throughout the planning and research process to produce research that benefits society and does as little harm as possible, as well as mindfulness for what happens after the research is complete and published. Researchers need to consider how their work will resolve unanswered questions and who the research might help, as well as consider how others might use or misuse their data, code, and results in the future (Lipworth, Mason, and Kerridge 2017; Teoli and Ghassemzadeh 2021). 5.1.2 Considerations before Ethical research should involve consideration of how data should be collected, so that certain individuals are not left out of reaping the benefits of important research. For example, women, non-binary individuals, disabled individuals, and people of certain ethnic backgrounds, and intersections of various demographic factors have been historically left out of clinical trials or when included, their data was inadequately recorded (Clark et al. 2019). For example, clinical trials often have questions about sex or gender with limited binary options (overlooking people without a binary sex and non-binary gendered individuals) resulting in a lack of collection of important information that could impact clinical outcomes, research results, and communication about results (Chen et al. 2019). Beyond this, even basic studies have historically often neglected to evaluate female animal models which can provide a greater understanding of how the research may successfully translate to more individuals. Yet another example is the historical lack of diversity in genomic reference datasets. To learn more about how social injustice, sexism, and other societal aspects have influenced bioethical and therefore data ethics practices, see Farmer (2004). 5.2 After Considerations While data sharing can result in wonderful opportunities for secondary analysis, we need to also consider some of the harm that could be caused by sharing our data and make sure that we do it mindfully. With more advanced forms of genomic and imaging technology, and increased use of data from our phones we have much more information (including real-time data) about the subjects we are using, and thus the risk to our subject from the consequences of others identifying the subjects in our studies is much higher than it used to be (Byrd et al. 2020). Overall there is a continuum of risk across the various types of data that we as researchers collect. Wile some forms of data, such as that derived from model organisms pose essentially no risk, intermediate forms of data such as summarized counts across a set of human samples pose more risk, while raw data and in particular data from individuals such as whole genome sequencing data, pose great risk for identification (Byrd et al. 2020). Note that recent technology advances in AI, show that chest X-ray images can now re-identify individuals (Packhäuser et al. (2022)). In addition, some histopathology images are also re-identifiable, see Ganz et al. (2025) for guidance about how to share images more safely. By the time you read these suggestions, they may be out-of-date or they may not be in alignment with institutional regulations, so please consult with experts at your organization. 5.2.1 Why does it mater that research subjects might be identifiable to others? In some cases open awareness about patients with certain types of cancers or diseases can be useful to allow other researchers and patients to find these individuals to encourage additional research and patient support group participation (especially for rare diseases or conditions). However, such information can put these individuals at risk for difficulty with insurance and employment, as well as at risk for other forms of discrimination. Furthermore, research data often also contains basic information about individuals, such as their address, which can be potentially deleterious for the safety of those individuals. New forms of research data from apps on our phone such as social media data collection, can pose more complicated risks based on data collection about the behaviors of research participants (Seh et al. 2020). Beyond the risk that data breaches pose to research participants, such breaches also cause harm to the research institutes where the breach occurred. Reputations and funding opportunities can be greatly compromised. Transparency and/or informed consent are discussed below as ways to mitigate these risks. 5.2.2 Why else does data protection matter at the individual level? If data gets manipulated or corrupted, this can result in false research findings, altered treatment plans by physicians, and more Seh et al. (2020). If patients are concerned that information will be used against them, there is some evidence that they are less likely to be forthcoming and honest with their providers. This poses concerns for data quality as well as trust in clinicians and health systems (Nong et al. 2022). Perpetuation of inequity is often cyclical. Considerations before research shape our options after research. For example, if people are excluded from the research process, data models are more likely to be biased against those populations. We will discuss what can be done to reduce the risks of research participants and others from your research. 5.3 Data ethics history To have an understanding of current theories about how to best deal with our research ethic conundrums it is helpful to be aware of the history of biomedical research in general. Most regulations of biomedical research stem from historical ill treatment of research participants. This has taken many forms from outright ill intent, to much more well-intended but neglectful research practices often due to a lack of awareness of the potential consequences. This has been especially difficult more recently as our potential to create and share data has dramatically expanded. 5.3.1 Historical incidences: Here are a couple of famous historical examples of medical research that were performed in a harmful manner. These incidences have shaped policy about ethical research, in terms of advocacy for informed consent (more on that in the next section), as well as recognition for the role of social injustice in research. Tuskegee syphilis trial: A study in Tuskegee, Alabama about the outcomes of untreated syphilis in Black males (1932-1972) in which the patients were told they were being treated but were in fact not being treated (McVean 2019). Henrietta Lacks and HeLa Cells: In 1951, a patient named Henrietta Lacks was treated for cervical cancer at Johns Hopkins. Her cancerous cells turned out to be uniquely capable of surviving and reproducing and have been used widely in research for decades for many discoveries. Her family did not receive money from the companies that profited from her cells, and for decades her family was often not asked for consent as doctors and scientists revealed her name and medical records publicly (“Henrietta Lacks: Science Must Right a Historical Wrong” (2020)). See here for additional examples. 5.4 Principles of Bioethics Several general concepts for Healthcare ethics, and by extension medical research ethics have been described in several commonly used ways, including the four pillars and the seven guiding principles. In the wake of medical and scientific abuses during WWII and beyond, several ethical principles and codes emerged. The Belmont Report (1979) defines the core bioethical pillars that drive ethical analysis in healthcare and research even today. 5.4.1 The four pillars (this discussion is from Melvin (2020)): The pillar of beneficence This pillar means that healthcare and its research must support the well-being of the patients. This includes reducing pain and helping to increase their overall quality of life. The pillar of non-maleficence Harm to patients must be minimized. This includes not causing intentional harm but also minimizing incompetency and accidental harm. This means for research studies and treatments, only conducting those where benefits outweigh the risks. The pillar of justice Healthcare must be provided to all equally and equitably regardless of race, gender, sex, socioeconomic class, religion, ethnicity, sexual orientation, age, or any other aspect of an individual’s identity. Equity specifically means that those who are societally disadvantaged should be even further aided. The pillar of autonomy (self-determination) Patients need to make their own decisions about their health. They should be equipped with all information about their health but then respected and supported in their decision. This is otherwise includes informed consent (Commissioner 2020), which means before a patient can truly consent, they need to be fully informed of the risks, ins and outs of any procedure, treatment, or research study participation. 5.4.2 The NIH Clinical Center Seven Principles The NIH published its “Guiding Principles for Ethical Research” (2015), which stem from the four pillars described above to provide a framework for ensuring the protection of people who volunteer for clinical research. 1)Social and clinical value The value of the study needs to be important enough to justify the discomfort, inconvenience that research participants may experience. 2)Scientific validity Studies should be designed so as to effectively gain more understanding about the scientific question in consideration. In other words, the efforts of the research participants should not be wasted on a study that is poorly designed and will not add to scientific understanding. 3)Fair subject selection Participants for studies should be selected based on the scientific question. Groups of people should not be excluded without a good reason. Participants should benefit from the understanding that the study should gain. In other words participants should not accept risk for only the benefit of others. 4)Favorable risk-benefit ratio Risks should be minimized, while benefits should be maximized. Risks are not just physical, but can involve mental, financial, social, or other risks. 5)Independent review To reduce conflicts of interest influencing the care of research participants, study plans should be reviewed by others who are trained to consider the ethical implications of such studies. 6)Informed consent Individuals should voluntarily decide if they wish to participate (where possible - some exceptions include very young children or those who are incapacitated “Informed Consent” (2023)). Participants should be informed about the risks (and potential uncertainty around those risks). This must be done in a way in which the individual can actually understand the information, for example it should be in a language that the individual understands. The information needs to be accurate and individuals should not feel coerced to participate. 7)Respect for potential and enrolled subjects Individuals should be treated with respect for the entirety of the process including: respecting the privacy of their information respecting their right to change their mind, including providing them any new information about risks or benefits that might cause them to change their mind respecting their welfare and providing treatment if needed and removing individuals for their welfare if needed respecting their welfare and their right to knowledge by letting them know what was learned from the research 5.5 Ethical Principles for Data These guidelines are also very useful for ensuring inclusive, transparent, open, and respectful data management practices: CARE Principles for Indigenous Data Governance, which largely focus on the self-determination of indigenous people and the usage of their data, as well as consideration for the impact and purpose of data: C stands for: Collective Benefit A stands for: Authority to Control R stands for: Responsibility E stands for: Ethics FAIR Principles aim to promote open data sharing: F stands for: Findable A stands for: Accessible I stands for: Interoperable R stands for: Reusable It is encouraged to consider both the CARE and FAIR principles together. 5.6 Concept of Consent We have already talked about the concept of informed consent. Obtaining consent should also include the following elements (based on “IOWA STATE UNIVERSITY Institutional Review BoardRecruitment of Research Participants” (2015) and “Informed Consent” (2023) and the author’s thoughts): Individuals should not feel pressured and should have adequate time to make the decision. Individuals should not experience undue influence, be coerced or be manipulated - They should not feel pressured by the individual recruiting, such as a boss or someone else of power or by offers in exchange for participation that would sway the decision. Individuals should not receive a misleading or overstated presentation of the potential benefits of the study. Individuals should be made aware of the uncertainty associated with risks and benefits Individuals should have the capacity to understand the risks and benefits (this involves consideration for language barriers, intellectual capacity, emotional capacity, stress, sleep loss and other forms of physical strain) Individuals should be able to withdraw consent at anytime Individuals should be respected throughout the process including consideration for the cultural values of the recruited populations Consent forms and processes should be reviewed by people with diverse expertise, such as understanding of ethics, equity, and patients and community experience 5.7 Medical Ethics Timeline It is helpful to get a sense of the timing when society established ethical medical standards and laws. Here we will point out important events in the timeline of medical ethics, with an emphasis on the United States. See here for a more in-depth timeline. 5.7.1 The Hippocratic Oath (~ 4th century BC) The concept of medical ethics in the Western world dates back all the way to the original Hippocratic Oath between the 3rd and 5th century BC (“Hippocratic Oath” 2023). It established concepts like confidentiality and non-maleficence. ” What I may see or hear in the course of the treatment or even outside of the treatment in regard to the life of men, which on no account one must spread abroad, I will keep to myself, holding such things shameful to be spoken about.”(“Hippocratic Oath” 2023) 5.7.2 American Medical Association Code of Medical Ethics (1847) The United States American Medical Association code of ethics was first established in 1874 (Riddick 2003). The next major code publication was in 1957. The code was not law, but it set standards for care. The 1957 code describe the fact that there might be special cases in which confidence might not always be able to be kept. See here by Higgins (1989) and here by Moskop et al. (2005) for more about the history of medical ethics codes. 5.7.3 Declaration of Helsinki (1964) The Declaration of Helsinki was published by the World Medical Association (WMA) and is considered “the world’s most widely recognized ethical principle for medical research involving humans” (Kurihara et al. 2024). It describes a set of principles for “medical research involving human subjects, including research on identifiable human material and data.” It has been amended several times and the WMA aims to keep it up to date. It outlines that research subjects welfare is the priority, that they have a right to self determination and the right to informed consent. Risks and benefits should be carefully considered and research should be discontinued if risks are determined to be to high (Association 1964). 5.7.4 International Covenant on Civil and Political Rights (1966) The United Nations adopted the concept of “free consent” (similar to informed consent) into international law (“Informed Consent” 2023). 5.7.5 Beecher’s Ethics and clinical research (1966) In 1966, Henry Beecher published an article called “Ethics and clinical research”, outlining serious ethical issues in biomedical research at the time. This encouraged the creation of additional guidelines (Beecher 1966; Stark 2016). 5.7.6 The Belmont Report (1979) The Belmont Report was written to describe guidelines for human subjects in biomedical and behavioral research. The report aims to provide a general framework for ethical consideration of research. It states that: These principles cannot always be applied so as to resolve beyond dispute particular ethical problems. The objective is to provide an analytical framework that will guide the resolution of ethical problems arising from research involving human subjects (“The Belmont Report,” n.d.). Here we briefly describe some of the major aspects of the report (“The Belmont Report,” n.d.). There are 3 ethical principles defined: Respect for Persons People should be allowed autonomy to use their judgment to make decisions for themselves. Those that cannot make all decisions for themselves, such as children or those who are incapacitated should be protected. Beneficence Harm to human subjects should be minimized and benefits should be maximized. Justice Benefits and burdens of research should be distributed equally. Justice demands both that these not provide advantages only to those who can afford them and that such research should not unduly involve persons from groups unlikely to be among the beneficiaries of subsequent applications of the research (“The Belmont Report,” n.d.) The application of these principles should involve the following: Informed Consent Consent should involve: information, comprehension, and voluntariness. Assessment of Risks and Benefits Potential risks and benefits should be thoroughly evaluated, including if human subjects are truly necessary. Benefits and risks must be “balanced” and shown to be “in a favorable ratio.” (“The Belmont Report,” n.d.) Selection of Subjects There must be fair procedures and outcomes in the selection of research subjects. Less burdened individuals should be called upon first to take on research burdens. Individuals who might be in conditions where they might be utilized for research more readily (such as those who are incarcerated or institutionalized), should be protected. 5.7.7 Health Insurance Portability and Accountability Act (HIPAA) (1996) Medical confidentially became law in the United States. Protected health information and identifiable health information must not be shared with anyone outside of certain covered entities without consent. Covered entities include: clinicians, insurance companies, and health care government agencies. 5.7.8 Genetic Information Nondiscrimination Act (GINA) (2008) The Genetic Information Nondiscrimination Act prohibits employers and health insurance companies from using genetic information to discriminate against individuals. 5.7.9 Health Information Technology for Economic and Clinical Health (HITECH) Act (2009) HITECH builds on HIPAA to add protections for electronic PHI as this became more common practice in health care with three basic new rules (Security 2021): Privacy Rule - Access should be limited to as few individuals as possible Security Rule - Safeguards should be implemented to protect electronic PHI, including technical means and physical means Breach Notification - Individuals should be notified in a timely manner about breaches that may have involved their PHI 5.8 Causes of research misconduct A research study by Davis, Riske-Morris, and Diaz (2007) investigated closed cases from the Office of Research Integrity (ORI), using statements from the reports to evaluate reasons why researchers sometimes do not perform research responsibly. A variety of reasons emerged that the researchers described including: Personal and professional stressors pressure to produce overworked insufficient time mental health issues overcommitted lack of support stress Organizational climate factors insufficient mentoring/supervision poor communication/coordination substandard lab procedures lost stolen data Job Insecurities These cases described situations in which a researcher felt hesitant to ask for help due to a perception of job insecurity. Inappropriate responsibility Competition for position Language Barrier Rationalizations Lack of control over environment Disseminating findings too quickly Personal Inhibitions Difficult job/task Frustrations Another category of Rationalizations Fear Apathy/Dislike Personality traits Impatience Laziness Personal Need for Recognition The article states that pressures to do public good can also be viewed as a personality factor since it seems indicative of dogma that can compromise the integrity of the scientific process, thus resulting in research misconduct. While personality traits were identified, the above reasons suggest that if researchers have more support to recognize and overcome stressors and are motivated to do quality research instead of attempting to obtain metrics for promotion, perhaps some of these reasons can be mitigated. 5.9 Consequences of research misconduct Research misconduct either due to malicious intent or unintentional neglect can have far reaching consequences. This section is based on Davis, Riske-Morris, and Diaz (2007) and National Academies of Sciences et al. (2017). 5.9.1 The Researcher According to a study of misconduct cases in the Office of Research Integrity (ORI), the investigator is always the accused and are scrutinized by their institution and federal agencies. If found guilty, they face many consequences including possible debarment, job loss, revoked degrees, revoked awards, and lawsuits . 5.9.2 The Institute and Journals Institutes and Journals can also face consequences of research misconduct, as they face reputation loses, which may reduce future opportunities for additional funding or support. 5.9.3 The Field Misconduct can lead to false findings that can lead to many other researchers investigating hypotheses that are not founded on true evidence. This can lead to wasted time, effort, and resources. 5.9.4 Patients Not only does misconduct lead to costs associated with a researcher and or institute losing some of their capacity to provide important positive research impacts to future patients, but it also reduces patient trust in research overall. 5.10 Misconduct prevention Several models have been proposed to reduce misconduct (Mousavi and Abdollahi 2020; Kumar 2010). They center on evaluating the pressures in academic evaluation and advancement and avoiding using simple quantitative metrics, such as the number of publications that a researcher publishes, with out more nuanced evaluation. They point out that a lack of experience or knowledge can also increase the risk of misconduct. It is suggested that the quality of research be prioritized over quantity. This would reduce pressures on academics to obtain flashy looking metrics and instead focus on doing work with the utmost integrity and rigor in a system where evaluations could better value these qualities. 5.11 Summary In summary, we have covered the following concepts in this chapter: Data ethics involves the ethical collection, storage, sharing and use of data to minimize harm and maximize benefits to research participants and society. Data ethics considerations should happen both before and after research is conducted. Researchers need to consider how their data could be misused after publication. Historical examples of unethical research highlight the need for data ethics principles, like informed consent and respect for research participants. Examples include the Tuskegee Syphilis Study and the case of Henrietta Lacks. Key principles of data ethics include: beneficence, non-maleficence, justice, autonomy and informed consent. Medical ethics has evolved over time with milestones like the Hippocratic Oath, AMA code of ethics, Nuremberg Code and establishment of laws like HIPAA, GINA and HITECH. Research misconduct can happen due to personal, organizational and job related factors. It can negatively impact researchers, institutes, research fields, patients and public trust. Misconduct prevention involves prioritizing research quality over quantity and reducing pressures that promote misconduct. Providing mentoring, supervision and support can also help prevent misconduct. References "],["current-data-concerns.html", "Chapter 6 Current Data Concerns 6.1 Current Ethical Issues 6.2 Recent Incidences 6.3 Summary", " Chapter 6 Current Data Concerns Given rapid changes in technology, data ethics is also rapidly evolving. Now we cover some of the current concerns for cancer research. 6.1 Current Ethical Issues There are several current issues that researchers and research participants, and really all individuals engaging in health care face. We are facing new and bigger ethical issues in data due to: Increasing practice of reusing data (and in ways that are new), which means consent processes cannot fully describe risks and benefits Large scale data collection which sometimes doesn’t involve consent, often this is done by companies for other reasons (for example, to create an AI tool that can distinguish images, a company may by coincidence also collect images of faces) New data sharing technologies provides increasing opportunities for security failures We will discuss some of these new issues briefly. 6.1.1 Consent for Data Reuse One major current ethical issue that we face now, is the consequences of the reuse of shared data. As we have described, there are major benefits of sharing data. It can allow researchers to really maximize their efforts. However, there are also negative potential consequences as well. Furthermore, it is still unclear what is exactly possible with our data, for both good and bad uses, as technology continues to advance. Previous management strategies for informed consent originate from research that predates the large scale data sharing that we now use today. In those cases, informed consent was a bit more straight forward to achieve. Now that data is often reused more often, it is often less obvious how data will be used for research purposes in the future, thus it is less obvious how to inform potential research participants what participation really means. Ideally we want to protect participants and family, while also maximizing the research potential of useful data. So how do we do this? Although ethical guidelines about this type of consent are evolving as research and technology evolve, here are some current methods for consent as described in McKeown et al. (2021). blanket consent - Subjects agree that their data can be used for any purpose that the data holder agrees is reasonable. broad consent - Subjects agree that their data can be used for a set of specified purposes. This is more protective but also restricts some uses of data. dynamic consent - Subjects are asked case-by-case for others to use their data. This requires more burden on both the data managers and also the subjects, as they need to continually decide about providing consent or not. meta consent - Subjects get to choose what type of consent they prefer of the other 3 options. McKeown et al. (2021) suggests that we also need to be mindful of the following: Participants should be educated about the uncertainty of the possible future uses of the data. Participants should be able to withdraw their consent for reasons that have been identified as being reasonable. (This creates many more ethical considerations.) Those managing the data and those using the data behave in a trustworthy manner based on defined data management and use regulations (including protection of of data). Participants should be informed about how ethical decisions about their data are weighed based on the benefit to society vs. the risk to individuals and that this involves uncertainty as well. We would like to suggest that updated information should possibly be provided to participants about changes in awareness of potential data uses and changes in awareness of the potential benefit or lack of benefit of the data to society. 6.1.2 Data sovereignty Let’s take a slightly deeper dive into what data sovereignty really is. We have already described it as a concept of ownership of data, that the people who the data comes from determine or at least are highly involved in what happens with that data. This is important in the study of people who have been marginalized and historically mistreated in research or otherwise. However, the term data sovereignty can also be defined more broadly, such as the authority of governments to survey or use data from domestic or foreign sources, a concept that is also of important recent interest (Hummel et al. 2021). A review of recent literature by Hummel et al. (2021) indicates that data sovereignty is often defined in many ways including the ability, rights, or laws surrounding the control, flow, privacy, security, or use of data. The review also suggests that there are therefore several major ways in which data sovereignty can be realized: Self-determination - Promoting, honoring, and respecting the interests, values and culture of those that the data comes from (particularly when it comes to indigenous populations) by allowing for their participation in determining what happens with the data and how it is collected. Strategies should be used to ensure that the data is beneficial and responsive to the needs of those that the data comes from, as well as mindful of the potential consequences of their use and collection. Technical Consideration - IT architecture to ensure that the data is safe and used as intended. Transparency - People should know when data is being collected about them and what it is being used for. Legal Considerations - Some countries or nations have regulations about what can be done with data particularly when it comes to transmission to other nations. The authors of the review (Hummel et al. 2021) caution that the inconsistency with which data sovereignty is defined could lead to negative consequences if efforts are not made to define the term when it is used. For example, if someone’s concept of data sovereignty is strictly of a technical definition, then important aspects related to transparency or self-determination may be overlooked. They note that Indigenous data sovereignty or data sovereignty as it pertains to indigenous populations, is more more clearly defined, and could be used as a model for other uses of the term in other contexts. Also see the CARE Principles for Indigenous Data Governance, as previously described in the last chapter, for more information about methods to protect data sovereignty for indigenous populations. The meaning of the terminology is likely to evolve over time, however, this indicates the complexity of data handling ethics that we are currently encountering and will continue to encounter. 6.1.3 Finding Artifacts In some cases researchers will have “incidental findings,” outside of the scope of the intended research. These incidental findings reveal aspects about the potential health or genetic risk of an individual as an artifact of performing other research. This leads to the question of whether those individuals should be informed about these findings. Depending on the nature of the research, the potential for finding incidental findings will vary. The Secretary’s Advisory Committee on Human Research Protections (SACHRP) at the US Department of Health and Human Services (Protections (OHRP) 2017) offers guidelines about this topic. Furthermore, if the research requires FDA regulations, than there is more defined guidance and requirements about incidental findings. Yet, for other forms of research, the determination will occur mostly with the institutional IRB. Often the first suggested step is to determine how likely the research is to yield any actionable incidental findings. Actionable findings are those where the research subject could actually do something to about the finding to improve their health or reduce the risk of health concerns. However, it can be difficult to determine if such a finding will become actionable in the future, particularly when it comes to collecting genomic data. To be more mindful of future consequences, researchers could also ask their research participants if they would want to know about incidental findings if they were to become actionable in the future. 6.2 Recent Incidences With advances in technology allowing for cheaper and easier production of medical datasets more than ever before, we have seen the creation of databanks and other shared data resources. This has resulted in new ethical issues. Here are some examples that exemplify more current data ethics issues: 6.2.1 Data Misuse for Marketing Commercial use of data is yet another possible use of research data. There is one example in which such a situation may have occurred, although there sources about the incident are conflicting (Kramer 2019). ReMy Health is a data analytics company that processes raw patient prescription and insurance data and provides this data to other companies. It was using data from Surescripts, a prescription and health record data company and providing it in a processed form for Amazon’s PillPack (https://www.pillpack.com/), a prescription delivery service. ReMy Health or one of its customers was accused of providing unauthorized access of prescription and patient health insurance information, which was believed to be for pharmaceutical companies for marketing decisions about what medications to market (Chiruvella and Guddati 2021). Surescripts then decided to revoke access for ReMy health to their data, thus hindering access to PillPack. However, Surescripts who made the allegations against ReMy Health has also had complaints of being threatening toward other companies, so it is a bit unclear exactly what happened (Kramer 2019). However, ultimately this resulted in a difficult situation for patients to receive their prescriptions and illustrates how data breaches or misuse by a single party when the data is utilized by multiple parties can get complicated (Kramer 2019). Consent forms are now required to disclose the potential for commercialization of products, and institutions navigate relationships with commercial companies in different ways. 6.2.2 Data Breaches MyHeritage is a genetic testing company based in Israel that provides ancestry information to customers. In 2018, a security incident occurred in which an unauthorized user somehow acquired access to email addresses and password hash keys for over 92 million users. Although this was a very large data breach in terms of the number of users impacted, they believe that none of the genetic data actually got leaked to this unauthorized user (“MyHeritage Statement About a Cybersecurity Incident” 2018). This incident however highlights the concern that could happen if the cybercriminal had been more successful. See here for other examples of PHI data breaches. Places that report data breaches - based on @Seh et al. (2020): PRC Database (Consumer data established in 1992) HIPAA Journal (Examples of violations of HIPAA compliance and guides to avoid violation, established in 2009) Office for Civil Rights Department of Health and Human Services of the USA (OCR) reports (yearly reports on health care data since 2009) Ponemon Institute Reports (Data privacy and security issues and policies, established in 2002) Verizon-DBIR (yearly investigations by Verizon Enterprises, established in 2008) The Department of Health and Human Services now requires reporting of data breaches to affected individuals. Breaches of over 500 people need to be notified publicly. See here for more information. 6.2.3 Data mistakes and neglect Keith Baggerly is a well-known expert in what he calls “Forensic Bioinformatics”. He evaluates other studies to see if he can reproduce their work. He has in a few cases found some very important mistakes. Often the mistakes have to do with sample mix-ups, such as a shift in a table resulting mislabeled rows or columns. Although such simple mistakes seem minor, Keith has shown that this can result in major consequences. One rather well-known example is his evaluation (along with Kevin Coombes and Jing Wang) of several now retracted articles (see here and here) by Anil Potti regarding chemosensitivity of cancer cell lines based on gene expression signatures (K. A. Baggerly and Coombes 2009). First the forensic team discovered simple yet very consequential errors (sample case control labels were swapped) in Potti’s work. This lead to further investigations revealing further mistakes, as well as data falsification in several of Potti’s articles. Ultimately, the clinical trials that were initiated based on Potti’s work were terminated, several of Potti’s articles were retracted, and he was asked to resign from his position. See (national_academies_of_sciences_detailed_2017?) for more information about Potti’s case as well as 4 other similar cases. Keith Baggerly points out that simple errors do not need to lead to dramatic consequences and encourages investigators to be transparent in reporting their mistakes as early as possible. A good example of an investigator owning up to a mistake is that of Bob Carpenter, see here due to some “buggy evaluation code” as he states here. While pressures often make us feel a need to be perfect. To err is to be human, and taking responsibility for our research mistakes should become acceptable, common practice, and revered by the research field at large. Keith points out however, that it is currently difficult for researchers to find the time to deeply investigate the work of others and suggests that perhaps scientists at funding agencies could perform such forensic work to ensure the integrity of our scientific findings (K. Baggerly 2018). Another interesting example is an investigation by Karl Browman, who is also well-known for his “Forensic Bioinformatics”, work is a paper in which Karl and his colleagues evaluated sample mix-ups in a dataset with both genotype and expression data for 500 mice. The data for the mice did not meet similarity expectations across the two types of data for 18% of the mice (Broman et al. 2015). The authors also created an R package to help correct for such mix-ups. The author’s conclude the manuscript as: What is an acceptable error rate in a research study? And what laboratory procedures should be instituted to avoid such errors? There exist procedures to help protect against errors, both for genotypes (e.g., Huijsmans et al. 2007a,b) and for microarrays (Grant et al. 2003; Imbeaud and Auffray 2005; Walter et al. 2010), but they are not always put into practice. However, as the current study indicates, with expression genetic data, one can accommodate a high rate of errors provided that one applies appropriate procedures to detect and correct such errors. This ultimately suggests that where researchers have two compatible types of data that allow for checking for mix-ups, methods to evaluate and correct for such errors could be very beneficial. 6.2.4 Data falsification Although the previous example ultimately led to some discoveries of falsification, the majority of the discovery started with findings of simple mistakes. However, there are many other reported cases of researchers falsifying or modifying their data to improve their results. See here for examples of misconduct cases identified by the Office of Research Integrity at the US department of Health and Human Services. 6.2.5 Improper Data Reuse for Research This section is largely based on an article by Garrison (2013). A research group at Arizona State University (ASU) initially collected DNA data from individuals of the Havasupai Tribe to study risk for type 2 diabetes in 1989. These samples were then later used for other genetic studies, including studies about “schizophrenia risk, ethnic migration, and population inbreeding” (Garrison 2013). The Tribe did not want research to be done on these topics, as they could be stigmatizing for certain groups. The research participants did not feel that they consented to other research outside the scope of diabetes and this ultimately led the Tribe to file a lawsuit in 2004 against the researchers and the ASU board of regents. No ruling was made as there was a procedural error leading to the case being dismissed, however a settlement was reached, giving monetary compensation and then return of the genetic samples back to the Tribe. The Tribe also banned all ASU researchers from their reservation. This case pointed out the challenges of informed consent especially when documents are written in languages other than the native language of the participants. In this case the participants signed informed consent documents written in English that said samples would be used for “behavioral/medical problems”, yet in spoken word, the consent documents may have been described as indicating that the samples were to be used for diabetes specifically. When the researchers decided to study schizophrenia, they obtained Internal Review Board (IRB) approval based on the consent documents, and no additional conversations happened with the Tribe to ensure that the participants understood and consented to the samples being used for other types of studies. Another important component of native or tribal populations, is that some tribes consider all Biological materials to be sacred and therefore greater transparency about the details of how materials are to be used and greater opportunity to determine how unused materials should be disposed would be very valuable. This was a very informative case in terms of pointing out many overlooked concerns in data ownership, particularly of vulnerable populations. These concerns include a lack of consideration about the balance of benefit to researchers and others relative to the potential harm or lack of benefit to the research participants and a lack of clarity about how to properly consent populations. Due to this case (“After Havasupai Litigation, Native Americans Wary of Genetic Research” 2010) and other historical disrespectful, neglectful, or harmful research engagements (Garrison et al. 2019), subsequently many tribes became reluctant to participate in research. As many of these concerns continue, many remain justifiably reluctant. Over time this has more recently resulted in discussions about how research can be more respectful, transparent, and culturally responsive with equity and justice as the priority, with Indigenous, Tribal or Native scholars leading the way (Garrison et al. 2019). However more work is desperately needed to improve both health disparities and justice. Possible guidelines could include (based on Garrison et al. (2019), Garrison (2013), and the author’s thoughts): To better understand the needs and concerns of the populations to be studied, members of those populations and communities should be directly involved in the governance of biological data. This concept is called data sovereignty (Garrison et al. 2019). Population members should help determine how researchers work with and consent individuals. More direct participation of these members in the research or IRB processes itself is also beneficial. Where possible biological samples should be physically “owned” by the populations that the come from or at least considered as such, even when samples are taken to a lab for processing. Details about how the samples should be stored or disposed of, should be discussed with the study population. Researchers and IRBs should consider not only the health impacts and security of research participants, but also the potential social and community consequences. To better understand these possible consequences, members of the populations studied should again be directly involved in these discussions. Researchers and IRBS should consider more about the balance of the possible benefit to researchers and society versus the potential lack of benefit or harm that the research may cause the participants are the communities they come from - research should aim to be more beneficial to those participating. To better understand the needs and possible harmful consequences of research, members of the communities or populations should be involved in discussions. Ambiguity about consent should be reduced as much as possible to ensure that information is provided that is as transparent, culturally responsive, and clear to those who are consented. Participants may need to be contacted again to ensure that consent remains responsive and transparent. More targeted specific data uses may be more comfortable for some populations, and may therefore be worth potential later restriction. Many research groups have decided to use Broad Consent in which only a set of potential future activities are consented to reduce later concerns. However this can still result in discomfort and thus a lack of participation by populations that are concerned about the use of their biological materials. This can ultimately result in even deeper health disparities. Overall, it is very important that researchers and research institutions continue to discuss and work with members of the populations they wish to study to create more equitable, just, and culturally responsive consent processes. One article by Begay et al. (2020) about the history of genetic studies of the Diné (or Navajo nation), another population that has faced genetic data misuse, state that the authors: encourage researchers to consider cultural perspectives and traditional knowledge that has the potential to create stronger conclusions and better-informed, ethical, and respectful science. 6.3 Summary In summary, we covered the following concepts: The field of data ethics is rapidly evolving as technology continues to evolve. Previous strategies for consent are not always appropriate in today’s age of data sharing, where some de-identified data can possibly be re-identified in the future. There are challenges with obtaining appropriate consent for data that may be reused in ways that are not initially envisioned. Various consent strategies exist with trade-offs. Participants need to be informed of the limitations and uncertainty of future data uses. Data sovereignty can be defined as the concept that the people who the data comes from determine or at least are highly involved in what happens with that data. However the meaning of data sovereignty is still evolving. There are many cases of data breaches, mistakes, and even falsification in research. Forensic bioinformatics aims to verify if published research is correct. Simple data mistakes in biomedical research can have dramatic consequences. It is important that researchers let the public know as soon as they are aware of any mistakes. Some cases reveal harm from sharing data in ways that research participants did not truly consent to, especially for vulnerable populations. A focus on preserving sovereignty, evaluating benefit vs. harm, and being cultural responsive can help mitigate possible harm. References "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     In memory of James Taylor, who was instrumental in initiating this project.   Credits Names Pedagogy Lead Content Instructor Carrie Wright Content Contributors Candace Savonen (sections of Data Sharing, Data Security, Data Ethics, and Coursera/Leanpub quizzes), Kate Isaac (Coursera practice quizzes) Content Editors/Reviewers Candace Savonen, Jeff Leek, Jodyn Platt Content Directors Jeff Leek Content Consultant (General) Elana Fertig Content Consultants (REDCap section) Jennifer Durham Acknowledgments Production Content Publisher Kate Isaac Content Publishing Reviewers Candace Savonen Technical Course Publishing Engineer Carrie Wright Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright Art and Design Illustrator Carrie Wright Funding Funder National Cancer Institute (NCI) UE5 CA254170 Funding Staff Sandra Ormbrek, Shasta Nicholson   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2025-02-21 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.41 2024-10-16 [1] CRAN (R 4.3.2) ## bslib 0.6.1 2023-11-28 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## cli 3.6.2 2023-12-11 [1] RSPM (R 4.3.0) ## devtools 2.4.5 2022-10-11 [1] RSPM (R 4.3.0) ## digest 0.6.34 2024-01-11 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 0.23 2023-11-01 [1] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] RSPM (R 4.3.0) ## glue 1.7.0 2024-01-09 [1] RSPM (R 4.3.0) ## htmltools 0.5.7 2023-11-03 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.4 2023-12-06 [1] RSPM (R 4.3.0) ## httpuv 1.6.14 2024-01-26 [1] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.8 2023-12-04 [1] RSPM (R 4.3.0) ## knitr 1.48 2024-07-07 [1] CRAN (R 4.3.2) ## later 1.3.2 2023-12-06 [1] RSPM (R 4.3.0) ## lifecycle 1.0.4 2023-11-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) ## miniUI 0.1.1.1 2018-05-18 [1] RSPM (R 4.3.0) ## pkgbuild 1.4.3 2023-12-10 [1] RSPM (R 4.3.0) ## pkgload 1.3.4 2024-01-16 [1] RSPM (R 4.3.0) ## profvis 0.3.8 2023-05-02 [1] RSPM (R 4.3.0) ## promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) ## purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.12 2024-01-09 [1] RSPM (R 4.3.0) ## remotes 2.4.2.1 2023-07-18 [1] RSPM (R 4.3.0) ## rlang 1.1.4 2024-06-04 [1] CRAN (R 4.3.2) ## rmarkdown 2.25 2023-09-18 [1] RSPM (R 4.3.0) ## sass 0.4.8 2023-12-06 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM (R 4.3.0) ## shiny 1.8.0 2023-11-17 [1] RSPM (R 4.3.0) ## stringi 1.8.3 2023-12-11 [1] RSPM (R 4.3.0) ## stringr 1.5.1 2023-11-14 [1] RSPM (R 4.3.0) ## urlchecker 1.0.1 2021-11-30 [1] RSPM (R 4.3.0) ## usethis 2.2.3 2024-02-19 [1] RSPM (R 4.3.0) ## vctrs 0.6.5 2023-12-01 [1] RSPM (R 4.3.0) ## xfun 0.48 2024-10-03 [1] CRAN (R 4.3.2) ## xtable 1.8-4 2019-04-21 [1] RSPM (R 4.3.0) ## yaml 2.3.8 2023-12-11 [1] RSPM (R 4.3.0) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
